{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "from mpl_toolkits.axes_grid.inset_locator import inset_axes\n",
    "\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "import pymc3\n",
    "from scipy import stats\n",
    "import sklearn as skl\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def iso_skill(x, y, br):\n",
    "    unc=br*(1-br)\n",
    "    return ((y-br)**2 - (x-y)**2)/unc\n",
    "\n",
    "def get_ci_discrim(p,n):\n",
    "    # Get CI for ratios (sens/spec/accuracy)\n",
    "    z=1.96\n",
    "    q = 1 - p\n",
    "    ci_lower = (2 * n * p + z*z - 1 - z * math.sqrt(z*z - 2 - 1/n + 4 * p * (n * q + 1)))/(2 * (n + z*z))\n",
    "    ci_upper = (2 * n * p + z*z + 1 + z * math.sqrt(z*z + 2 - 1/n + 4 * p * (n * q - 1)))/(2 * (n + z*z))\n",
    "    se=(ci_upper-ci_lower)/3.92\n",
    "\n",
    "    return (ci_lower,ci_upper),se\n",
    "\n",
    "def get_ci_calib(stats):\n",
    "    # get 95% CIs for calibration metrics\n",
    "    alpha = 0.95\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    ci_lower = max(0.0, np.percentile(stats, p))\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    ci_upper = min(1.0, np.percentile(stats, p))\n",
    "    se=(ci_upper-ci_lower)/3.92\n",
    "\n",
    "    return (ci_lower, ci_upper),se\n",
    "\n",
    "def format_number(num):\n",
    "    if abs(num)<0.001:\n",
    "        x=\"0\"\n",
    "    else:\n",
    "        x=str(float('%.2g' % num))\n",
    "    return(x)\n",
    "\n",
    "def calibration_loss(truth,forecast):\n",
    "    # Calculates calibration loss and the Sanders score\n",
    "    # Returns the array (needs to be summed later to get the number)\n",
    "    \n",
    "    # Assumes we don't need to re-bin)\n",
    "    N=len(truth)\n",
    "    levels=list(set(forecast))\n",
    "    levels.sort()\n",
    "    K=len(levels)\n",
    "    calibration_loss_array=[]\n",
    "    sanders_array=[]\n",
    "\n",
    "    for pk in levels:\n",
    "        nk=np.count_nonzero(forecast == pk)\n",
    "        i=sum(np.where(forecast == pk))\n",
    "        obark=sum(truth[i])/nk\n",
    "        calibration_loss_array.append((nk/N)*((pk-obark)**2))\n",
    "        sanders_array.append( (pk-obark)**2 + obark*(1-obark))\n",
    "\n",
    "    return (calibration_loss_array,sanders_array)\n",
    "    \n",
    "\n",
    "def overconfidence_score(truth,forecast):\n",
    "    \n",
    "    # https://github.com/mlr-org/mlr/issues/842\n",
    "    # Convert to numpy arrays\n",
    "    forecast=np.array(forecast)\n",
    "    truth=np.array(truth)\n",
    "    \n",
    "    # Generate the half-range forecasts \n",
    "    # i.e., Fold the array at 0.5 \n",
    "    forecast_half_range=np.abs(forecast-0.5)+0.5\n",
    "    \n",
    "    # Get the forecast array ready for truth matching\n",
    "    forecast[np.where(forecast<0.5)[0]]=0\n",
    "    forecast[np.where(forecast>0.5)[0]]=1\n",
    "    \n",
    "    # Get the % correct\n",
    "    perc_correct=len(np.where(abs(forecast-truth)==0)[0])/len(truth)\n",
    "    \n",
    "    return(np.mean(forecast_half_range)-perc_correct)\n",
    "\n",
    "def plot_calib_results(truth, forecast, plot_ci=True, plot_isoskill=False, nplots=3, fig_title=\"\",n_bins=20):\n",
    "\n",
    "\n",
    "    # Get rid of too much precision in the forecasts \n",
    "    # We just need one decimal point here. This also helps\n",
    "    # with binning later\n",
    "    forecast=np.round(forecast,2)\n",
    "    \n",
    "    \n",
    "    #####################################\n",
    "    # Data for the plots\n",
    "    # Calibration Curve\n",
    "    calib_y, calib_x = calibration_curve(truth, forecast,n_bins=n_bins)\n",
    "    # ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(truth, forecast,pos_label=1)\n",
    "    #####################################\n",
    "    \n",
    "    #####################################\n",
    "    # Get 95% CI for the calibration plot\n",
    "    # Get a list of the bins of calib_y\n",
    "    #####################################\n",
    "    # Get the counts for each item in calib_x\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "    binids = np.digitize(forecast, bins) - 1\n",
    "    bin_forecasts = np.bincount(binids, minlength=len(bins))\n",
    "    bin_forecasts=bin_forecasts[bin_forecasts != 0]\n",
    "    ci=[]\n",
    "    i=0\n",
    "\n",
    "    for prob in calib_y:\n",
    "        p=prob\n",
    "        n=bin_forecasts[i]\n",
    "        AA=proportion_confint(count=n*p, nobs=n, alpha=0.05, method='binom_test')\n",
    "\n",
    "        ci.append(AA)\n",
    "        i=i+1\n",
    "    # Transpose ci and subtract from midpoint so it'll work with pyplot.errorbar\n",
    "    ci_adj=np.transpose(ci)\n",
    "    ci_adj[0,]=calib_y-ci_adj[0,]\n",
    "    ci_adj[1,]=ci_adj[1,]-calib_y\n",
    "    #####################################\n",
    "        \n",
    "    #####################################\n",
    "    # Get the metrics\n",
    "    base_rate=sum(truth)/len(truth)\n",
    "    uncertainty=base_rate*(1-base_rate)\n",
    "    sharpness=np.var(forecast)\n",
    "\n",
    "    # Calibration metrics\n",
    "    brier=skl.metrics.brier_score_loss(truth,forecast)\n",
    "    (temp_cl,sanders)=calibration_loss(truth=truth,forecast=forecast)\n",
    "    calibration=sum(temp_cl)\n",
    "    overconfidence=overconfidence_score(truth,forecast)\n",
    "    cil=np.mean(forecast)-np.mean(truth)\n",
    "    resolution=calibration+uncertainty-brier\n",
    "    refinement=brier-calibration\n",
    "    brier_base=uncertainty\n",
    "    brier_skill_score=-(brier-brier_base)/brier_base\n",
    "    min_bss=(uncertainty-1)/uncertainty\n",
    "    kappa=skl.metrics.cohen_kappa_score(truth, forecast.round())\n",
    "\n",
    "\n",
    "    # Discrimination metrics\n",
    "    AUC=skl.metrics.auc(fpr,tpr)\n",
    "    cm=skl.metrics.confusion_matrix(truth,forecast.round())\n",
    "    tp=cm[1,1]\n",
    "    tn=cm[0,0]\n",
    "    fp=cm[0,1]\n",
    "    fn=cm[1,0]\n",
    "    sensitivity=tp/(tp+fn)\n",
    "    specificity=tn/(tn+fp)\n",
    "    accuracy=(tp+tn)/(len(truth))\n",
    "    \n",
    "    # Get CI and SE using frequentist method\n",
    "    sensitivity_ci_f, sensitivity_se=get_ci_discrim(sensitivity,len(truth))\n",
    "    specificity_ci_f, specificity_se=get_ci_discrim(specificity,len(truth))\n",
    "    accuracy_ci_f, accuracy_se=get_ci_discrim(accuracy,len(truth))\n",
    "\n",
    "    ###############################\n",
    "    # Bootstrap CIs\n",
    "    # configure bootstrap\n",
    "    n_iterations = 1000\n",
    "\n",
    "    # Define arrays to collect bootstraps\n",
    "    uncertainty_a =list()\n",
    "    brier_a =list()\n",
    "    calibration_a =list()\n",
    "    overconfidence_a =list()\n",
    "    cil_a=list()\n",
    "    resolution_a=list()\n",
    "    refinement_a=list()\n",
    "    brier_skill_score_a=list()\n",
    "    auc_a=list()\n",
    "    kappa_a=list()\n",
    "\n",
    "    sensitivity_a=list()\n",
    "    specificity_a=list()\n",
    "    accuracy_a=list()\n",
    "\n",
    "\n",
    "    values=np.column_stack((forecast,truth))\n",
    "    data_size = len(truth) # find length of the data set\n",
    "    \n",
    "    for x in range(0, n_iterations-1):\n",
    "\n",
    "        # create an array of random indices equal to data_size\n",
    "        # this array will be used to sample with replacement from the arrays\n",
    "        idx = np.random.randint(data_size, size=data_size)\n",
    "        truth_s = truth[idx]\n",
    "\n",
    "        # sample from forecasts\n",
    "        forecast_s = forecast[idx]\n",
    "\n",
    "        base_rate_s=sum(truth_s)/len(truth_s)\n",
    "        uncertainty_s=base_rate_s*(1-base_rate_s)\n",
    "\n",
    "        if uncertainty_s>0:\n",
    "\n",
    "            brier_s=skl.metrics.brier_score_loss(truth_s,forecast_s)\n",
    "            (temp_cl_s,sanders_s)=calibration_loss(truth_s,forecast_s)\n",
    "            calibration_s=sum(temp_cl_s)\n",
    "            overconfidence_s=overconfidence_score(truth_s,forecast_s)\n",
    "            cil_s=np.mean(forecast_s)-np.mean(truth_s)\n",
    "            resolution_s=calibration_s+uncertainty_s-brier_s\n",
    "            refinement_s=brier_s-calibration_s\n",
    "            brier_base_s=uncertainty_s\n",
    "            brier_skill_score_s=-(brier_s-brier_base_s)/brier_base_s\n",
    "            kappa_s=skl.metrics.cohen_kappa_score(truth_s,forecast_s.round())\n",
    "\n",
    "            fpr_s, tpr_s, thresholds_s = roc_curve(truth_s, forecast_s,pos_label=1)\n",
    "            auc_s=skl.metrics.auc(fpr_s,tpr_s)\n",
    "\n",
    "            cm_s=skl.metrics.confusion_matrix(truth_s,forecast_s.round())\n",
    "            tp_s=cm[1,1]\n",
    "            tn_s=cm[0,0]\n",
    "            fp_s=cm[0,1]\n",
    "            fn_s=cm[1,0]\n",
    "            sensitivity_s=tp_s/(tp_s+fn_s)\n",
    "            specificity_s=tn_s/(tn_s+fp_s)\n",
    "            accuracy_s=(tp_s+tn_s)/(len(truth_s))\n",
    "            uncertainty_a.append(uncertainty_s)\n",
    "            brier_a.append(brier_s)\n",
    "            calibration_a.append(calibration_s)\n",
    "            overconfidence_a.append(overconfidence_s)\n",
    "            cil_a.append(cil_s)\n",
    "            resolution_a.append(resolution_s)\n",
    "            refinement_a.append(refinement_s)\n",
    "            brier_skill_score_a.append(brier_skill_score_s)\n",
    "            auc_a.append(auc_s)\n",
    "            sensitivity_a.append(sensitivity_s)\n",
    "            specificity_a.append(specificity_s)\n",
    "            accuracy_a.append(accuracy_s)\n",
    "            kappa_a.append(kappa_s)\n",
    "\n",
    "        else:\n",
    "            x=x-1\n",
    "\n",
    "    # Get CI and SE using frequentist method\n",
    "    brier_ci,brier_se=get_ci_calib(brier_a)\n",
    "    calibration_ci,calibration_se=get_ci_calib(calibration_a)\n",
    "    overconfidence_ci,overconfidence_se=get_ci_calib(overconfidence_a)\n",
    "    cil_ci,cil_se=get_ci_calib(cil_a)\n",
    "    resolution_ci,resolution_se=get_ci_calib(resolution_a)\n",
    "    refinement_ci,refinement_se=get_ci_calib(refinement_a)\n",
    "    brier_skill_score_ci,brier_skill_score_se=get_ci_calib(brier_skill_score_a)\n",
    "    auc_ci,auc_se=get_ci_calib(auc_a)\n",
    "    kappa_ci,kappa_se=get_ci_calib(kappa_a)\n",
    "\n",
    "    \n",
    "    # Get CI using 95%HPD\n",
    "    uncertainty_ci=pymc3.stats.hpd(np.array(uncertainty_a))\n",
    "    brier_ci=pymc3.stats.hpd(np.array(brier_a))\n",
    "    calibration_ci=pymc3.stats.hpd(np.array(calibration_a))\n",
    "    overconfidence_ci=pymc3.stats.hpd(np.array(overconfidence_a))\n",
    "    resolution_ci=pymc3.stats.hpd(np.array(resolution_a))\n",
    "    refinement_ci=pymc3.stats.hpd(np.array(refinement_a))\n",
    "    brier_skill_score_ci=pymc3.stats.hpd(np.array(brier_skill_score_a))\n",
    "    auc_ci=pymc3.stats.hpd(np.array(auc_a))\n",
    "    sensitivity_ci=pymc3.stats.hpd(np.array(sensitivity_a))\n",
    "    specificity_ci=pymc3.stats.hpd(np.array(specificity_a))\n",
    "    accuracy_ci=pymc3.stats.hpd(np.array(accuracy_a))\n",
    "    kappa_ci=pymc3.stats.hpd(np.array(kappa_a))\n",
    "\n",
    "\n",
    "    # Get Zombie plot ellipse coordinates\n",
    "    ellipse_width=specificity_ci_f[1]-specificity_ci_f[0]\n",
    "    ellipse_height=sensitivity_ci_f[1]-sensitivity_ci_f[0]\n",
    "    ellipse_center=[]\n",
    "    ellipse_center=[1-(specificity_ci_f[0]+ellipse_width/2),sensitivity_ci_f[0]+ellipse_height/2]\n",
    "\n",
    "    #####################################\n",
    "    \n",
    "    if plot_isoskill:\n",
    "        # Get equal skill lines\n",
    "        ls = np.linspace(0.0,1.0,100)\n",
    "        X, Y = np.meshgrid(ls,ls)\n",
    "        Z = iso_skill(X,Y,base_rate)\n",
    "\n",
    "    #####################################\n",
    "    # Plots\n",
    "    # Set figure defaults\n",
    "    fig_height=6\n",
    "    fig_width=13\n",
    "\n",
    "    if (nplots==3):\n",
    "        f, axs = plt.subplots(1,3,figsize=(fig_width,fig_height))\n",
    "        title_size=14\n",
    "        axis_size=13\n",
    "        tick_size=12\n",
    "        marker_size=7\n",
    "        line_width=2\n",
    "        auc_font_size=12\n",
    "        sup_y=0.9\n",
    "    else:\n",
    "        f, axs = plt.subplots(1,2,figsize=(fig_width,fig_height))\n",
    "        title_size=18\n",
    "        axis_size=16\n",
    "        tick_size=14\n",
    "        marker_size=10\n",
    "        line_width=2.5\n",
    "        auc_font_size=16\n",
    "        sup_y=1.03\n",
    "\n",
    "    f.suptitle(fig_title, fontsize=24, ha=\"center\",y=sup_y)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot the confidence  calibration curve\n",
    "    axs[0].errorbar(calib_x,calib_y,yerr=ci_adj,fmt='o-',capsize=4,markersize=marker_size,linewidth=line_width)\n",
    "\n",
    "    # Plot the diagnonal\n",
    "    axs[0].plot([0,1],[0,1],'k-',linewidth=line_width/2)\n",
    "    # Plot the base rate\n",
    "    axs[0].plot([0,1],[base_rate,base_rate],'k--',linewidth=line_width/2)\n",
    "    # Plot the skill area\n",
    "    x1=[0,base_rate,1]\n",
    "    y1=[base_rate/2., base_rate,(1+base_rate)/2.]\n",
    "    axs[0].fill_between(x1[0:2],0,y1[0:2], color=\"green\",alpha=0.1)\n",
    "    axs[0].fill_between(x1[1:3],y1[1:3],1, color=\"green\",alpha=0.1)\n",
    "\n",
    "    # Plot lines of optimal resolution\n",
    "    axs[0].plot([0,base_rate],[0,0],'g:')\n",
    "    axs[0].plot([base_rate,1],[1,1],'g:')\n",
    "\n",
    "    # Plot lines of optimal accuracy\n",
    "    epsilon=0.0\n",
    "    axs[0].plot([0,0.5],[0+epsilon,0+epsilon],'g-.')\n",
    "    axs[0].plot([0.5,1],[1-epsilon,1-epsilon],'g-.')\n",
    "\n",
    "    # Plot isoskill lines if needed\n",
    "    if plot_isoskill:\n",
    "        # Find minimum skill given the prevalence. \n",
    "        # Used to get a reasonable range of contours\n",
    "        min_skill=round((uncertainty-1)/uncertainty,1)\n",
    "\n",
    "        # Set floor at -10, since it can get to -inf at low uncertainty\n",
    "        if (min_skill<-10):\n",
    "            min_skill=-10\n",
    "\n",
    "        # Get a rounded step size from min skill to 0\n",
    "        interval=(0-min_skill)/5\n",
    "        # Create a range from min_skill to zero\n",
    "        neg_interval=np.round(np.arange(min_skill,0,interval),0)\n",
    "        # Add positive range of skill from 0 to 1, in increments of 0.2\n",
    "        combined_interval=np.unique(np.concatenate((neg_interval,np.arange(0,1.1,0.2))))\n",
    "\n",
    "        # Create the iso-contour lines and add to axs[0]\n",
    "        contours = axs[0].contour(X,Y,Z, combined_interval, colors=\"green\", alpha=0.7)\n",
    "        axs[0].clabel(contours, inline=True, fontsize=8)\n",
    "\n",
    "\n",
    "    #########################################################################################\n",
    "    # Plot the sharpness histogram\n",
    "    # Set location of sharpness histogram under the confidence calibration curve\n",
    "    if nplots==3:\n",
    "        ax_inset=inset_axes(axs[0], width=\"100%\",  height=\"40%\", bbox_to_anchor=(-0.025,-0.7,1,.65), loc=2, bbox_transform=axs[0].transAxes)\n",
    "    else:\n",
    "        ax_inset=inset_axes(axs[0], width=\"100%\",  height=\"40%\", bbox_to_anchor=(-0.015,-0.7,1,.65), loc=2, bbox_transform=axs[0].transAxes)\n",
    "    \n",
    "    # Set the width of the bins. If there's only one bin and set the width at 0.1\n",
    "    if (len(calib_x)>1):\n",
    "        bar_width=min([0.1,min(np.ediff1d(calib_x))])\n",
    "    else:\n",
    "        bar_width=0.1\n",
    "\n",
    "    # Plot the sharpness histogram\n",
    "    ax_inset.bar(calib_x,bin_forecasts,width=bar_width)\n",
    "\n",
    "    # Print the text labels for the sharpness histogram\n",
    "    for i in range(len(calib_x)):\n",
    "        ax_inset.text(calib_x[i], bin_forecasts[i] + max(bin_forecasts)*0.07, str(bin_forecasts[i]), color='black',ha='center')\n",
    "    \n",
    "    # Set labels and limits for the confidence calibration curve and sharpness histogram\n",
    "    axs[0].set_title('Confidence Calibration Curve',fontsize=title_size)\n",
    "    axs[0].set_ylabel('Observed probability of event',fontsize=axis_size)\n",
    "    axs[0].yaxis.set_label_coords(-0.15,0.5)\n",
    "    axs[0].set_aspect('equal', adjustable='box')\n",
    "    axs[0].set_xlim(-0.05,1.05)\n",
    "    axs[0].set_ylim(-0.05,1.05)\n",
    "    axs[0].set_xticklabels([])\n",
    "    axs[0].tick_params(axis=\"x\", labelsize=tick_size)\n",
    "    axs[0].tick_params(axis=\"y\", labelsize=tick_size)\n",
    "    ax_inset.set_xlabel('Predicted chance of event',fontsize=axis_size)\n",
    "    ax_inset.set_ylabel('# predictions',fontsize=axis_size,labelpad=133)\n",
    "    ax_inset.set_xlim(-0.05,1.05)\n",
    "    ax_inset.set_ylim(0,max(bin_forecasts)*1.35)\n",
    "    ax_inset.yaxis.set_label_coords(-0.15,0.5)\n",
    "    ax_inset.tick_params(axis=\"x\", labelsize=tick_size)\n",
    "    ax_inset.tick_params(axis=\"y\", labelsize=tick_size)\n",
    "\n",
    "    ###########################################################################\n",
    "    # Plot the ROC\n",
    "    axs[1].plot(fpr, tpr,'o-',markersize=marker_size,linewidth=line_width)\n",
    "    # Plot the diagnonal\n",
    "    d_x=[0, 0.33333, 1]\n",
    "    d_y=[0, 0.33333, 1]\n",
    "    axs[1].plot(d_x, d_y,'k--',linewidth=line_width/2)\n",
    "    # Shift ROC subplot to make it look nicer\n",
    "    box = axs[1].get_position()\n",
    "    box.x0 = box.x0 + 0.01\n",
    "    box.x1 = box.x1 + 0.01\n",
    "    axs[1].set_position(box)\n",
    "\n",
    "    # Plot the zombie plot areas\n",
    "    z_x1=[0, 0.33333,1]\n",
    "    z_y1=[0, 1,1]\n",
    "    z_x2=[0,1,1]\n",
    "    z_y2=[0.6666,1,1]\n",
    "    axs[1].fill_between(z_x1,z_y1, d_y ,color=\"red\",alpha=0.1)\n",
    "    axs[1].fill_between(z_x2,z_y2, 0,color=\"red\",alpha=0.1)\n",
    "    axs[1].fill_between(d_x,d_y, 0,color=\"red\",alpha=0.3)\n",
    "    auc_text='AUC='+str(AUC.round(3))\n",
    "    axs[1].text(0.48,0.25,auc_text,fontsize=auc_font_size)\n",
    "    axs[1].add_artist(Ellipse(ellipse_center, ellipse_width, ellipse_height,color=\"black\",alpha=0.1))\n",
    "    axs[1].scatter(1-specificity,sensitivity,marker=\"s\",s=marker_size*15,color=\"black\")\n",
    "\n",
    "    # Set labels and limits for ROC curve\n",
    "    axs[1].set_title('ROC Curve',fontsize=title_size)\n",
    "    axs[1].set_xlabel('1-Specificity',fontsize=axis_size)\n",
    "    axs[1].set_ylabel('Sensitivity',fontsize=axis_size)\n",
    "    axs[1].set_aspect('equal', adjustable='box')\n",
    "    axs[1].tick_params(axis=\"x\", labelsize=tick_size)\n",
    "    axs[1].tick_params(axis=\"y\", labelsize=tick_size)\n",
    "\n",
    "\n",
    "    #####################################################################################################################\n",
    "    # Put the output variables in a dict\n",
    "    #  Format: Name, value, 95%HDI low, 95%HDI high, SE, range low, range high, explanation, direction of improved performance\n",
    "    metrics=dict([('Prevalence', (base_rate,False,False,False,0,1,\"Pre-test probability based on truth.\",False)), \n",
    "                  ('Uncertainty', (uncertainty,uncertainty_ci[0],uncertainty_ci[1],False,0,0.25,\"Lower indicates an easier diagnostic problem.\",False)), \n",
    "                  ('Sharpness', (sharpness,False,False,False,0,0.25,\"Measure of variance of the forecasts. No relationship to quality of forecasts.\",False)),\n",
    "                  ('Calibration loss', (calibration,calibration_ci[0],calibration_ci[1],calibration_se,0,1,\"How well forecasts line up with the diagonal. Lower is better.\",\"low\")), \n",
    "                  ('Confidence score', (overconfidence,overconfidence_ci[0],overconfidence_ci[1],overconfidence_se,0,1,\"Degree of confidence. Negative and positive values indicate under- and over-confidence, respectively. A score of 0 is best \",\"low\")), \n",
    "                  ('Calibration-in-large', (cil,cil_ci[0],cil_ci[1],cil_se,0,1,\"Mean of forecasts-mean of outcomes. A score of 0 is best \",\"low\")), \n",
    "                  ('Resolution', (resolution,resolution_ci[0],resolution_ci[1],resolution_se,0,round(uncertainty,2),\"How well the forecasts are differentiated from the prevalence. Higher is better. Upper value =  uncertainty\",\"high\")), \n",
    "                  ('Refinement loss', (refinement,refinement_ci[0],refinement_ci[1],refinement_se,0,round(uncertainty,2),\"How poorly predictions discriminate between the presence and absence of disease. Lower is better. Upper limit = uncertainty.\",\"low\")),\n",
    "                  ('Cohen Kappa', (kappa,kappa_ci[0],kappa_ci[1],kappa_se,0,1,\"Agreement between forecast and truth after accounting for randomness. Upper is better.\",\"high\")),\n",
    "                  ('Brier score loss', (brier,brier_ci[0],brier_ci[1],brier_se, 0,1,\"Measure of the error between predictions and truth. Lower is better.\",\"low\")), \n",
    "                  ('Brier skill score', (brier_skill_score,brier_skill_score_ci[0],brier_skill_score_ci[1],brier_skill_score_se,round(min_bss,2),1,\"Measures the skill of the predictor compared to guessing based on pre-test probability. Lowest (worst) value is a function of uncertainty and ranges from -infinity at uncertainty of 0 to -3 at uncertainty of 0.25. Higher is better.\",\"high\")), \n",
    "                  ('Sensitivity', (sensitivity,sensitivity_ci_f[0],sensitivity_ci_f[1],sensitivity_se,0,1,\"True positive rate. Measures the proportion of actual positives that are correctly identified as such. Higher is better.\",\"high\")), \n",
    "                  ('Specificity', (specificity,specificity_ci_f[0],specificity_ci_f[1],specificity_se,0,1,\"True negative rate. Measures the proportion of actual negatives that are correctly identified as such. Higher is better.\",\"high\")), \n",
    "                  ('Accuracy', (accuracy,accuracy_ci_f[0],accuracy_ci_f[1],accuracy_se, 0,1,\"True classification rate. Measures the proportion of actual positive and negatives that are correctly identified as such. Higher is better.\",\"high\")), \n",
    "                  ('AUC',(AUC,auc_ci[0],auc_ci[1],auc_se,0,1,\"Area under the ROC curve. Probability of ranking a randomly chosen positive case higher than a randomly chosen negative one. Higher is better.\",\"high\"))\n",
    "])\n",
    "    \n",
    "    if (nplots==3):\n",
    "        # Generate text block for plot3 \n",
    "        h='Metric\\t\\t\\tValue\\t95%HDI/CI\\n'\n",
    "        h=h+'-----------------------------------------\\n'\n",
    "        t=''\n",
    "        for m in metrics:  \n",
    "            temp_array=metrics[m]\n",
    "            tab_1=\"\\t\"\n",
    "            if len(m)<16:\n",
    "                tab_1=tab_1+\"\\t\"\n",
    "                if len(m)<4:\n",
    "                    tab_1=tab_1+\"\\t\"\n",
    "            if temp_array[2]==False:\n",
    "                t=t+m+tab_1+format_number(temp_array[0])+\"\\n\"\n",
    "            else:\n",
    "                t=t+m+tab_1+format_number(temp_array[0])+\"\\t\"+format_number(temp_array[1])+\" - \"+ format_number(temp_array[2])+\"\\n\"\n",
    "        \n",
    "        t=t+'-----------------------------------------'\n",
    "\n",
    "        # Print text on plot 3 and turn off its axes\n",
    "        axs[2].axis('off')\n",
    "        axs[2].set_aspect('equal', adjustable='box')\n",
    "        axs[2].text(-0.10,0.85,h.expandtabs(),fontproperties='monospace',fontsize=9,fontweight='bold',verticalalignment='bottom')\n",
    "        axs[2].text(-0.10,0.9,t.expandtabs(),fontproperties='monospace',fontsize=9,verticalalignment='top')\n",
    "        axs[2].tick_params(axis=\"x\", labelsize=tick_size)\n",
    "        axs[2].tick_params(axis=\"y\", labelsize=tick_size)\n",
    "\n",
    "\n",
    "    return (plt,metrics,cm)\n",
    "\n",
    "\n",
    "#####################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
